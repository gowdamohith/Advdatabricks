{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18dbde16-ddbd-49c6-ad09-feca9fd256a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n| id| name|salary|\n+---+-----+------+\n|  1|Alice| 50000|\n|  2|  Bob| 60000|\n+---+-----+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from delta.tables import DeltaTable\n",
    "# Create a Spark session with Delta Lake support\n",
    "spark = SparkSession.builder.appName(\"DeltaLakeDemo\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "# Create a Delta Table\n",
    "data = [(1, \"Alice\", 50000), (2, \"Bob\", 60000)]\n",
    "columns = [\"id\", \"name\", \"salary\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(\"/mnt/delta/employee\")\n",
    "# Read and display the table\n",
    "df_delta = spark.read.format(\"delta\").load(\"/mnt/delta/employee\")\n",
    "df_delta.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60e8e162-0ea2-42b7-906d-2b9194d4e263",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n| id| name|salary|\n+---+-----+------+\n|  1|Alice| 60000|\n|  2|  Bob| 70000|\n+---+-----+------+\n\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"delta\").load(\"/mnt/delta/employee\")\n",
    "df = df.withColumn(\"salary\", df.salary + 5000)\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(\"/mnt/delta/employee\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "643c3502-aad2-46c6-a61c-4b3dbd74c432",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+\n| id|   name|salary|\n+---+-------+------+\n|  3|Charlie| 70000|\n|  1|  Alice| 55000|\n|  2|    Bob| 65000|\n+---+-------+------+\n\n"
     ]
    }
   ],
   "source": [
    "from delta.tables import *\n",
    "deltaTable = DeltaTable.forPath(spark, \"/mnt/delta/employee\")\n",
    "deltaTable.alias(\"old\").merge(\n",
    "    spark.createDataFrame([(3, \"Charlie\", 70000)], [\"id\", \"name\", \"salary\"]).alias(\"new\"),\n",
    "    \"old.id = new.id\"\n",
    ").whenNotMatchedInsert(values={\"id\": \"new.id\", \"name\": \"new.name\", \"salary\": \"new.salary\"}) \\\n",
    ".execute()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bc82a76-f8a8-41f1-afa5-8431de7ec153",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+\n| id|   name|salary|\n+---+-------+------+\n|  3|Charlie| 70000|\n|  1|  Alice| 55000|\n|  2|    Bob| 65000|\n+---+-------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"delta\").option(\"versionAsOf\", 3).load(\"/mnt/delta/employee\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "197e5542-932e-46bb-ae4c-d453aaaaba25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+----------------+------------------------+---------+--------------------------------------------------------------------------------------------------------------------------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n|version|timestamp          |userId          |userName                |operation|operationParameters                                                                                                                                     |job |notebook          |clusterId           |readVersion|isolationLevel   |isBlindAppend|operationMetrics                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |userMetadata|engineInfo                         |\n+-------+-------------------+----------------+------------------------+---------+--------------------------------------------------------------------------------------------------------------------------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n|3      |2025-03-18 09:34:31|2034898594157119|mohithgowda265@gmail.com|MERGE    |{predicate -> [\"(id#2468L = id#2474L)\"], matchedPredicates -> [], notMatchedPredicates -> [{\"actionType\":\"insert\"}], notMatchedBySourcePredicates -> []}|null|{3219724976096886}|0318-090519-zet9epzz|2          |WriteSerializable|false        |{numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 0, numTargetBytesAdded -> 0, numTargetBytesRemoved -> 0, numTargetDeletionVectorsAdded -> 0, numTargetRowsMatchedUpdated -> 0, executionTimeMs -> 3497, materializeSourceTimeMs -> 6, numTargetRowsInserted -> 0, numTargetRowsMatchedDeleted -> 0, scanTimeMs -> 0, numTargetRowsUpdated -> 0, numOutputRows -> 0, numTargetDeletionVectorsRemoved -> 0, numTargetRowsNotMatchedBySourceUpdated -> 0, numTargetChangeFilesAdded -> 0, numSourceRows -> 1, numTargetFilesRemoved -> 0, numTargetRowsNotMatchedBySourceDeleted -> 0, rewriteTimeMs -> 3375}    |null        |Databricks-Runtime/12.2.x-scala2.12|\n|2      |2025-03-18 09:28:49|2034898594157119|mohithgowda265@gmail.com|MERGE    |{predicate -> [\"(id#1412L = id#1418L)\"], matchedPredicates -> [], notMatchedPredicates -> [{\"actionType\":\"insert\"}], notMatchedBySourcePredicates -> []}|null|{3219724976096886}|0318-090519-zet9epzz|1          |WriteSerializable|false        |{numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 1, numTargetBytesAdded -> 1110, numTargetBytesRemoved -> 0, numTargetDeletionVectorsAdded -> 0, numTargetRowsMatchedUpdated -> 0, executionTimeMs -> 4296, materializeSourceTimeMs -> 18, numTargetRowsInserted -> 1, numTargetRowsMatchedDeleted -> 0, scanTimeMs -> 0, numTargetRowsUpdated -> 0, numOutputRows -> 1, numTargetDeletionVectorsRemoved -> 0, numTargetRowsNotMatchedBySourceUpdated -> 0, numTargetChangeFilesAdded -> 0, numSourceRows -> 1, numTargetFilesRemoved -> 0, numTargetRowsNotMatchedBySourceDeleted -> 0, rewriteTimeMs -> 4090}|null        |Databricks-Runtime/12.2.x-scala2.12|\n|1      |2025-03-18 09:24:24|2034898594157119|mohithgowda265@gmail.com|WRITE    |{mode -> Overwrite, partitionBy -> []}                                                                                                                  |null|{3219724976096886}|0318-090519-zet9epzz|0          |WriteSerializable|false        |{numFiles -> 2, numOutputRows -> 2, numOutputBytes -> 2178}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |null        |Databricks-Runtime/12.2.x-scala2.12|\n|0      |2025-03-18 09:22:29|2034898594157119|mohithgowda265@gmail.com|WRITE    |{mode -> Overwrite, partitionBy -> []}                                                                                                                  |null|{3219724976096886}|0318-090519-zet9epzz|null       |WriteSerializable|false        |{numFiles -> 2, numOutputRows -> 2, numOutputBytes -> 2178}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |null        |Databricks-Runtime/12.2.x-scala2.12|\n+-------+-------------------+----------------+------------------------+---------+--------------------------------------------------------------------------------------------------------------------------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE HISTORY delta.`/mnt/delta/employee`\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41a16184-8349-4799-8c77-5630f6d48550",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Define Delta table path\n",
    "delta_table_path = \"/mnt/delta/employee\"\n",
    "\n",
    "# Read the existing Delta table\n",
    "df = spark.read.format(\"delta\").load(delta_table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9df5e677-ca75-489e-9587-06745aed6612",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Repartition data to optimize file layout\n",
    "df_repartitioned = df.repartition(4)  # Adjust partition count based on data size\n",
    "\n",
    "# Sort the data manually by the Z-Order column (e.g., 'salary')\n",
    "df_sorted = df_repartitioned.sort(\"salary\")\n",
    "\n",
    "# Overwrite the existing Delta table with optimized layout\n",
    "df_sorted.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8183a35-955c-4dec-bdcf-341386c9bd58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+----------------+------------------------+---------+--------------------------------------------------------------------------------------------------------------------------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n|version|timestamp          |userId          |userName                |operation|operationParameters                                                                                                                                     |job |notebook          |clusterId           |readVersion|isolationLevel   |isBlindAppend|operationMetrics                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |userMetadata|engineInfo                         |\n+-------+-------------------+----------------+------------------------+---------+--------------------------------------------------------------------------------------------------------------------------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n|4      |2025-03-18 11:35:29|2034898594157119|mohithgowda265@gmail.com|WRITE    |{mode -> Overwrite, partitionBy -> []}                                                                                                                  |null|{3219724976096886}|0318-090519-zet9epzz|3          |WriteSerializable|false        |{numFiles -> 1, numOutputRows -> 3, numOutputBytes -> 1129}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |null        |Databricks-Runtime/12.2.x-scala2.12|\n|3      |2025-03-18 09:34:31|2034898594157119|mohithgowda265@gmail.com|MERGE    |{predicate -> [\"(id#2468L = id#2474L)\"], matchedPredicates -> [], notMatchedPredicates -> [{\"actionType\":\"insert\"}], notMatchedBySourcePredicates -> []}|null|{3219724976096886}|0318-090519-zet9epzz|2          |WriteSerializable|false        |{numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 0, numTargetBytesAdded -> 0, numTargetBytesRemoved -> 0, numTargetDeletionVectorsAdded -> 0, numTargetRowsMatchedUpdated -> 0, executionTimeMs -> 3497, materializeSourceTimeMs -> 6, numTargetRowsInserted -> 0, numTargetRowsMatchedDeleted -> 0, scanTimeMs -> 0, numTargetRowsUpdated -> 0, numOutputRows -> 0, numTargetDeletionVectorsRemoved -> 0, numTargetRowsNotMatchedBySourceUpdated -> 0, numTargetChangeFilesAdded -> 0, numSourceRows -> 1, numTargetFilesRemoved -> 0, numTargetRowsNotMatchedBySourceDeleted -> 0, rewriteTimeMs -> 3375}    |null        |Databricks-Runtime/12.2.x-scala2.12|\n|2      |2025-03-18 09:28:49|2034898594157119|mohithgowda265@gmail.com|MERGE    |{predicate -> [\"(id#1412L = id#1418L)\"], matchedPredicates -> [], notMatchedPredicates -> [{\"actionType\":\"insert\"}], notMatchedBySourcePredicates -> []}|null|{3219724976096886}|0318-090519-zet9epzz|1          |WriteSerializable|false        |{numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 1, numTargetBytesAdded -> 1110, numTargetBytesRemoved -> 0, numTargetDeletionVectorsAdded -> 0, numTargetRowsMatchedUpdated -> 0, executionTimeMs -> 4296, materializeSourceTimeMs -> 18, numTargetRowsInserted -> 1, numTargetRowsMatchedDeleted -> 0, scanTimeMs -> 0, numTargetRowsUpdated -> 0, numOutputRows -> 1, numTargetDeletionVectorsRemoved -> 0, numTargetRowsNotMatchedBySourceUpdated -> 0, numTargetChangeFilesAdded -> 0, numSourceRows -> 1, numTargetFilesRemoved -> 0, numTargetRowsNotMatchedBySourceDeleted -> 0, rewriteTimeMs -> 4090}|null        |Databricks-Runtime/12.2.x-scala2.12|\n|1      |2025-03-18 09:24:24|2034898594157119|mohithgowda265@gmail.com|WRITE    |{mode -> Overwrite, partitionBy -> []}                                                                                                                  |null|{3219724976096886}|0318-090519-zet9epzz|0          |WriteSerializable|false        |{numFiles -> 2, numOutputRows -> 2, numOutputBytes -> 2178}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |null        |Databricks-Runtime/12.2.x-scala2.12|\n|0      |2025-03-18 09:22:29|2034898594157119|mohithgowda265@gmail.com|WRITE    |{mode -> Overwrite, partitionBy -> []}                                                                                                                  |null|{3219724976096886}|0318-090519-zet9epzz|null       |WriteSerializable|false        |{numFiles -> 2, numOutputRows -> 2, numOutputBytes -> 2178}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |null        |Databricks-Runtime/12.2.x-scala2.12|\n+-------+-------------------+----------------+------------------------+---------+--------------------------------------------------------------------------------------------------------------------------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df_history = spark.sql(\"DESCRIBE HISTORY delta.`/mnt/delta/employee`\")\n",
    "df_history.show(truncate=False)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "advdatabricks_day1",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
